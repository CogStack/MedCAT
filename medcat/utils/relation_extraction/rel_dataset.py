from ast import literal_eval
from typing import Any, Iterable, List, Dict, Tuple, Union
from torch.utils.data import Dataset
from spacy.tokens import Doc
import logging
import pandas
import random
import torch
from medcat.cdb import CDB
from medcat.config_rel_cat import ConfigRelCAT
from medcat.utils.meta_cat.data_utils import Span
from medcat.utils.relation_extraction.tokenizer import TokenizerWrapperBERT


class RelData(Dataset):

    name = "rel_dataset"

    log = logging.getLogger(__name__)

    def __init__(self, tokenizer: TokenizerWrapperBERT, config: ConfigRelCAT, cdb: CDB = CDB()):
        """ Use this class to create a dataset for relation annotations from CSV exports,
            MedCAT exports or Spacy Documents (assuming the documents got generated by MedCAT,
            if they did not then please set the required paramenters manually to match MedCAT output,
            see /medcat/cat.py#_add_nested_ent)

            If you are using this to create relations from CSV it is assumed that your entities/concepts of
            interest are surrounded by the special tokens, see create_base_relations_from_csv doc.

        Args:
            tokenizer (TokenizerWrapperBERT): okenizer used to generate token ids from input text
            config (ConfigRelCAT): same config used in RelCAT
            cdb (CDB): Optional, used to add concept ids and types to detected ents, 
                useful when creating datasets from MedCAT output. Defaults to CDB().
        """

        self.cdb: CDB = cdb
        self.config: ConfigRelCAT = config
        self.tokenizer: TokenizerWrapperBERT = tokenizer
        self.dataset: Dict[Any, Any] = {}

        self.log.setLevel(self.config.general.log_level)

    def generate_base_relations(self, docs: Iterable[Doc]) -> List[Dict]:
        """ Util function, should be used if you want to train from spacy docs

        Args:
            docs (Iterable[Doc]): Generate relations from Spacy CAT docs.

        Returns:
            output_relations: List[Dict] : []      
                "output_relations": relation_instances, <-- see create_base_relations_from_doc/csv
                                                            for data columns
                "nclasses": self.config.model.padding_idx, <-- dummy class
                "labels2idx": {}, 
                "idx2label": {}}
                ]
        """

        output_relations = []
        for doc_id, doc in enumerate(docs):
            output_relations.append(
                self.create_base_relations_from_doc(doc, doc_id=str(doc_id),))

        return output_relations

    def create_base_relations_from_csv(self, csv_path: str):
        """
            Assumes the columns are as follows ["relation_token_span_ids", "ent1_ent2_start", "ent1", "ent2", "label",
            "label_id", "ent1_type", "ent2_type", "ent1_id", "ent2_id", "ent1_cui", "ent2_cui", "doc_id", "sents"],
            last column is the actual source text.

            The entities inside the text MUST be annotated with special tokens i.e:
                ...some text..[s1] first entity [e1].....[s2] second entity [e2]........ 
            You have to store the start position, aka index position of token [e1] and also of token [e2] in
            the (ent1_ent2_start) column.

        Args:
            csv_path (str): path to csv file, must have specific columns, tab separated,

        Returns:
            Dict : {  
                "output_relations": relation_instances, <-- see create_base_relations_from_doc/csv
                                                            for data columns
                "nclasses": self.config.model.padding_idx, <-- dummy class
                "labels2idx": {}, 
                "idx2label": {}}
            }
        """

        df = pandas.read_csv(csv_path, index_col=False,
                             encoding='utf-8', sep="\t")

        tmp_col_rel_token_col = df.pop("relation_token_span_ids")

        df.insert(0, "relation_token_span_ids", tmp_col_rel_token_col)

        text_cols = ["sents", "text"]

        df["ent1_ent2_start"] = df["ent1_ent2_start"].apply(
            lambda x: literal_eval(str(x)))

        for col in text_cols:
            if col in df.columns:
                out_rels = []
                for row_idx in range(len(df[col])):
                    _text = df.iloc[row_idx][col]
                    _ent1_ent2_start = df.iloc[row_idx]["ent1_ent2_start"]
                    _rels = self.create_base_relations_from_doc(
                        _text, doc_id=str(row_idx), ent1_ent2_tokens_start_pos=_ent1_ent2_start,)
                    out_rels.append(_rels)

                rows_to_remove = []
                for row_idx in range(len(out_rels)):
                    if len(out_rels[row_idx]["output_relations"]) < 1:
                        rows_to_remove.append(row_idx)

                relation_token_span_ids = []
                out_ent1_ent2_starts = []

                for rel in out_rels:
                    if len(rel["output_relations"]) > 0:
                        relation_token_span_ids.append(
                            rel["output_relations"][0][0])
                        out_ent1_ent2_starts.append(
                            rel["output_relations"][0][1])
                    else:
                        relation_token_span_ids.append([])
                        out_ent1_ent2_starts.append([])

                df["label"] = [i.strip() for i in df["label"]]

                df["relation_token_span_ids"] = relation_token_span_ids
                df["ent1_ent2_start"] = out_ent1_ent2_starts

                df = df.drop(index=rows_to_remove)
                df = df.drop(columns=col)
                break

        nclasses, labels2idx, idx2label = RelData.get_labels(
            df["label"], self.config)

        output_relations = df.values.tolist()

        self.log.info("CSV dataset | No. of relations detected:" + str(len(output_relations)) +
                      "| from : " + csv_path + " | nclasses: " + str(nclasses) + " | idx2label: " + str(idx2label))

        self.log.info("Samples per class: ")
        for label_num in list(idx2label.keys()):
            sample_count = 0
            for output_relation in output_relations:
                if label_num == output_relation[5]:
                    sample_count += 1
            self.log.info(
                " label: " + idx2label[label_num] + " | samples: " + str(sample_count))

        # replace/update label_id with actual detected label number
        for idx in range(len(output_relations)):
            output_relations[idx][5] = labels2idx[output_relations[idx][4]]

        return {"output_relations": output_relations, "nclasses": nclasses, "labels2idx": labels2idx, "idx2label": idx2label}

    def create_base_relations_from_doc(self, doc: Union[Doc, str], doc_id: str, ent1_ent2_tokens_start_pos: Union[List, Tuple] = (-1, -1)) -> Dict:
        """  Creates a list of tuples based on pairs of entities detected (relation, ent1, ent2) for one spacy document or text string.

        Args:
            doc (Union[Doc, str]): SpacyDoc or string of text, each will get handled slightly differently
            doc_id (str): document id
            ent1_ent2_tokens_start_pos (Union[List, Tuple], optional): start of [s1][s2] tokens, if left default
                    we assume we are dealing with a SpacyDoc. Defaults to (-1, -1).

        Returns:
                Dict : {  
                    "output_relations": relation_instances, <-- see create_base_relations_from_doc/csv
                                                                for data columns
                    "nclasses": self.config.model.padding_idx, <-- dummy class
                    "labels2idx": {}, 
                    "idx2label": {}}
                }
        """
        relation_instances = []

        chars_to_exclude = ":!@#$%^&*()-+?_=.,;<>/[]{}"
        tokenizer_data = None

        if isinstance(doc, str):
            tokenizer_data = self.tokenizer(doc, truncation=False)
            doc_text = doc
        elif isinstance(doc, Doc):
            tokenizer_data = self.tokenizer(doc.text, truncation=False)
            doc_text = doc.text

        doc_length = len(tokenizer_data["tokens"])

        if ent1_ent2_tokens_start_pos != (-1, -1):
            ent1_token_start_pos, ent2_token_start_pos = ent1_ent2_tokens_start_pos[0],\
                ent1_ent2_tokens_start_pos[1]
            # add + 1 to the pos cause of [CLS]
            if self.config.general.annotation_schema_tag_ids:
                ent1_token_start_pos, ent2_token_start_pos = ent1_ent2_tokens_start_pos[0] + 1,\
                    ent1_ent2_tokens_start_pos[1] + 1

            ent1_start_char_pos, _ = tokenizer_data["offset_mapping"][ent1_token_start_pos]
            ent2_start_char_pos, _ = tokenizer_data["offset_mapping"][ent2_token_start_pos]

            if abs(ent2_start_char_pos - ent1_start_char_pos) <= self.config.general.window_size:

                ent1_left_ent_context_token_pos_end = ent1_token_start_pos - \
                    self.config.general.cntx_left

                left_context_start_char_pos = 0
                right_context_start_end_pos = len(doc_text) - 1

                if ent1_left_ent_context_token_pos_end < 0:
                    ent1_left_ent_context_token_pos_end = 0
                else:
                    left_context_start_char_pos = tokenizer_data[
                        "offset_mapping"][ent1_left_ent_context_token_pos_end][0]

                ent2_right_ent_context_token_pos_end = ent2_token_start_pos + \
                    self.config.general.cntx_right

                # get end of 2nd ent token (if using tags)
                if self.config.general.annotation_schema_tag_ids:
                    far_pos = -1
                    for tkn_id in self.config.general.annotation_schema_tag_ids:
                        pos = [i for i in range(
                            0, doc_length) if tokenizer_data["input_ids"][i] == tkn_id][0]
                        far_pos = pos if far_pos < pos else far_pos
                    ent2_right_ent_context_token_pos_end = far_pos

                if ent2_right_ent_context_token_pos_end >= doc_length - 1:
                    ent2_right_ent_context_token_pos_end = doc_length - 2
                else:
                    right_context_start_end_pos = tokenizer_data[
                        "offset_mapping"][ent2_right_ent_context_token_pos_end][1]

                ent1_token = tokenizer_data["tokens"][ent1_token_start_pos]
                ent2_token = tokenizer_data["tokens"][ent2_token_start_pos]

                window_tokenizer_data = self.tokenizer(
                    doc_text[left_context_start_char_pos:right_context_start_end_pos])

                # update token loc to match new selection
                if self.config.general.annotation_schema_tag_ids:
                    ent1_token_start_pos = \
                        window_tokenizer_data["input_ids"].index(
                            self.config.general.annotation_schema_tag_ids[0])
                    ent2_token_start_pos = \
                        window_tokenizer_data["input_ids"].index(
                            self.config.general.annotation_schema_tag_ids[2])
                else:
                    ent2_token_start_pos = ent2_token_start_pos - ent1_token_start_pos
                    ent1_token_start_pos = self.config.general.cntx_left if ent1_token_start_pos - \
                        self.config.general.cntx_left > 0 else ent1_token_start_pos
                    ent2_token_start_pos += ent1_token_start_pos

                ent1_ent2_new_start = (
                    ent1_token_start_pos, ent2_token_start_pos)

                en1_start, en1_end = window_tokenizer_data["offset_mapping"][ent1_token_start_pos]
                en2_start, en2_end = window_tokenizer_data["offset_mapping"][ent2_token_start_pos]

                relation_instances.append([window_tokenizer_data["input_ids"], ent1_ent2_new_start, ent1_token, ent2_token, "UNK", self.config.model.padding_idx,
                                           None, None, None, None, None, None, doc_id, "",
                                           en1_start, en1_end, en2_start, en2_end])

        elif isinstance(doc, Doc):

            _ents = doc.ents if len(doc.ents) > 0 else doc._.ents
            for ent1_idx in range(0, len(_ents) - 1):

                ent1_token: Span = _ents[ent1_idx]   # type: ignore

                if str(ent1_token) not in chars_to_exclude:
                    ent1_type_id = list(
                        self.cdb.cui2type_ids.get(ent1_token._.cui, ''))
                    ent1_types = [self.cdb.addl_info['type_id2name'].get(
                        tui, '') for tui in ent1_type_id]

                    ent2pos = ent1_idx + 1

                    ent1_start = ent1_token.start
                    ent1_end = ent1_token.end

                    # get actual token index from the text
                    _ent1_token_idx = [i for i in range(len(tokenizer_data["offset_mapping"])) if ent1_start in
                                       range(
                                           tokenizer_data["offset_mapping"][i][0], tokenizer_data["offset_mapping"][i][1] + 1)
                                       or ent1_end in range(tokenizer_data["offset_mapping"][i][0], tokenizer_data["offset_mapping"][i][1] + 1)
                                       ][0]

                    left_context_start_char_pos = 0
                    ent1_left_ent_context_token_pos_end = _ent1_token_idx - self.config.general.cntx_left

                    if ent1_left_ent_context_token_pos_end < 0:
                        ent1_left_ent_context_token_pos_end = 0
                    else:
                        left_context_start_char_pos = tokenizer_data[
                            "offset_mapping"][ent1_left_ent_context_token_pos_end][0]

                    for ent2_idx in range(ent2pos, len(_ents)):
                        ent2_token: Span = _ents[ent2_idx]   # type: ignore

                        if ent2_token in _ents:
                            if str(ent2_token) not in chars_to_exclude and str(ent1_token) != str(ent2_token):
                                ent2_type_id = list(
                                    self.cdb.cui2type_ids.get(ent2_token._.cui, ''))
                                ent2_types = [self.cdb.addl_info['type_id2name'].get(
                                    tui, '') for tui in ent2_type_id]

                                ent2_start = ent2_token.start
                                ent2_end = ent2_token.end
                                if ent2_start - ent1_start <= self.config.general.window_size and ent2_start - ent1_start > 0:
                                    _ent2_token_idx = [i for i in range(len(tokenizer_data["offset_mapping"])) if ent2_start in
                                                       range(
                                                           tokenizer_data["offset_mapping"][i][0], tokenizer_data["offset_mapping"][i][1] + 1)
                                                       or ent2_end in
                                                       range(
                                                           tokenizer_data["offset_mapping"][i][0], tokenizer_data["offset_mapping"][i][1] + 1)
                                                       ][0]

                                    right_context_start_end_pos = len(
                                        doc_text) - 1
                                    ent2_right_ent_context_token_pos_end = _ent2_token_idx + \
                                        self.config.general.cntx_right

                                    if ent2_right_ent_context_token_pos_end >= doc_length - 1:
                                        ent2_right_ent_context_token_pos_end = doc_length - 2
                                    else:
                                        right_context_start_end_pos = tokenizer_data[
                                            "offset_mapping"][ent2_right_ent_context_token_pos_end][1]

                                    tmp_doc_text = doc_text

                                    # check if a tag is present, and if not so then insert the custom annotation tags in
                                    if self.config.general.annotation_schema_tag_ids[0] not in tokenizer_data["input_ids"]:
                                        _pre_e1 = tmp_doc_text[0: (ent1_start)]
                                        _e1_s2 = tmp_doc_text[(
                                            ent1_end): (ent2_start)]
                                        _e2_end = tmp_doc_text[(
                                            ent2_end): len(doc_text)]
                                        _ent2_token_idx = (_ent2_token_idx + 2)

                                        annotation_token_text = self.tokenizer.hf_tokenizers.convert_ids_to_tokens(
                                            self.config.general.annotation_schema_tag_ids)

                                        tmp_doc_text = _pre_e1 + " " + \
                                            annotation_token_text[0] + " " + \
                                            str(ent1_token) + " " + \
                                            annotation_token_text[1] + " " + _e1_s2 + " " + \
                                            annotation_token_text[2] + " " + str(ent2_token) + " " + \
                                            annotation_token_text[3] + \
                                            " " + _e2_end

                                        ann_tag_token_len = len(
                                            annotation_token_text[0])

                                        _left_context_start_char_pos = left_context_start_char_pos - ann_tag_token_len
                                        left_context_start_char_pos = 0 if _left_context_start_char_pos <= 0 \
                                            else _left_context_start_char_pos

                                        right_context_start_end_pos = right_context_start_end_pos if right_context_start_end_pos >= len(tmp_doc_text) \
                                            else right_context_start_end_pos + (ann_tag_token_len * 4)

                                    window_tokenizer_data = self.tokenizer(
                                        tmp_doc_text[left_context_start_char_pos:right_context_start_end_pos])

                                    if self.config.general.annotation_schema_tag_ids:
                                        ent1_token_start_pos = \
                                            window_tokenizer_data["input_ids"].index(
                                                self.config.general.annotation_schema_tag_ids[0])
                                        ent2_token_start_pos = \
                                            window_tokenizer_data["input_ids"].index(
                                                self.config.general.annotation_schema_tag_ids[2])
                                    else:
                                        ent2_token_start_pos = _ent2_token_idx - _ent1_token_idx if _ent1_token_idx - \
                                            self.config.general.cntx_left > 0 else _ent2_token_idx
                                        ent1_token_start_pos = self.config.general.cntx_left if _ent1_token_idx - \
                                            self.config.general.cntx_left > 0 else _ent1_token_idx
                                        ent2_token_start_pos += ent1_token_start_pos

                                    ent1_ent2_new_start = (
                                        ent1_token_start_pos, ent2_token_start_pos)

                                    en1_start, en1_end = window_tokenizer_data[
                                        "offset_mapping"][ent1_token_start_pos]
                                    en2_start, en2_end = window_tokenizer_data[
                                        "offset_mapping"][ent2_token_start_pos]

                                    relation_instances.append([window_tokenizer_data["input_ids"], ent1_ent2_new_start, ent1_token, ent2_token, "UNK", self.config.model.padding_idx,
                                                               ent1_types, ent2_types, ent1_token._.id, ent2_token._.id, ent1_token._.cui, ent2_token._.cui, doc_id, "",
                                                               en1_start, en1_end, en2_start, en2_end])

        return {"output_relations": relation_instances, "nclasses": self.config.model.padding_idx, "labels2idx": {}, "idx2label": {}}

    def create_relations_from_export(self, data: Dict):
        """  
            Args:
                data (Dict):
                    MedCAT Export data.

            Returns:
                Dict : {  
                    "output_relations": relation_instances, <-- see create_base_relations_from_doc/csv
                                                                for data columns
                    "nclasses": self.config.model.padding_idx, <-- dummy class
                    "labels2idx": {}, 
                    "idx2label": {}}
                }
        """

        output_relations = []

        relation_type_filter_pairs = self.config.general.relation_type_filter_pairs

        annotation_token_text = self.tokenizer.hf_tokenizers.convert_ids_to_tokens(
            self.config.general.annotation_schema_tag_ids)

        for project in data['projects']:
            for doc_id, document in enumerate(project['documents']):
                text = str(document['text'])
                if len(text) > 0:
                    annotations = document['annotations']
                    relations = document['relations']

                    if self.config.general.lowercase:
                        text = text.lower()

                    tokenizer_data = self.tokenizer(text, truncation=False)

                    doc_length_tokens = len(tokenizer_data["tokens"])

                    relation_instances = []
                    ann_ids_from_reliations = []

                    ann_ids_ents: Dict[Any, Any] = {}

                    _other_rel_subset = []

                    for ent1_idx, ent1_ann in enumerate(annotations):
                        ann_id = ent1_ann['id']
                        ann_ids_ents[ann_id] = {}
                        ann_ids_ents[ann_id]['cui'] = ent1_ann['cui']
                        ann_ids_ents[ann_id]['type_ids'] = list(
                            self.cdb.cui2type_ids.get(ent1_ann['cui'], ''))
                        ann_ids_ents[ann_id]['types'] = [self.cdb.addl_info['type_id2name'].get(
                            tui, '') for tui in ann_ids_ents[ann_id]['type_ids']]

                        if self.config.general.mct_export_create_addl_rels:

                            for _, ent2_ann in enumerate(annotations[ent1_idx + 1:]):
                                if abs(ent1_ann["start"] - ent2_ann["start"]) <= self.config.general.window_size:
                                    if ent1_ann["validated"] and ent2_ann["validated"]:
                                        _other_rel_subset.append({
                                            "start_entity": ent1_ann["id"],
                                            "start_entity_cui": ent1_ann["cui"],
                                            "start_entity_value": ent1_ann["value"],
                                            "start_entity_start_idx": ent1_ann["start"],
                                            "start_entity_end_idx": ent1_ann["end"],
                                            "end_entity": ent2_ann["id"],
                                            "end_entity_cui": ent2_ann["cui"],
                                            "end_entity_value": ent2_ann["value"],
                                            "end_entity_start_idx": ent2_ann["start"],
                                            "end_entity_end_idx": ent2_ann["end"],
                                            "relation": "Other",
                                            "validated": True
                                        })

                    non_rel_sample_size_limit = int(int(
                        self.config.general.mct_export_max_non_rel_sample_size) / len(data['projects']))

                    if non_rel_sample_size_limit > 0 and len(_other_rel_subset) > 0:
                        random.shuffle(_other_rel_subset)
                        _other_rel_subset = _other_rel_subset[0:non_rel_sample_size_limit]

                    relations.extend(_other_rel_subset)

                    for relation in relations:
                        ann_start_start_pos = relation['start_entity_start_idx']
                        ann_start_end_pos = relation["start_entity_end_idx"]

                        ann_end_start_pos = relation['end_entity_start_idx']
                        ann_end_end_pos = relation["end_entity_end_idx"]

                        start_entity_value = relation['start_entity_value']
                        end_entity_value = relation['end_entity_value']

                        start_entity_id = relation['start_entity']
                        end_entity_id = relation['end_entity']

                        start_entity_types = ann_ids_ents[start_entity_id]['types']
                        end_entity_types = ann_ids_ents[end_entity_id]['types']
                        start_entity_cui = ann_ids_ents[start_entity_id]['cui']
                        end_entity_cui = ann_ids_ents[end_entity_id]['cui']

                        # if somehow the annotations belong to the same relation but make sense in reverse
                        if ann_start_start_pos > ann_end_start_pos:
                            ann_end_start_pos = relation['start_entity_start_idx']
                            ann_end_end_pos = relation['start_entity_end_idx']

                            ann_start_start_pos = relation['end_entity_start_idx']
                            ann_start_end_pos = relation['end_entity_end_idx']

                            end_entity_value = relation['start_entity_value']
                            start_entity_value = relation['end_entity_value']

                            end_entity_cui = ann_ids_ents[start_entity_id]['cui']
                            start_entity_cui = ann_ids_ents[end_entity_id]['cui']

                            end_entity_types = ann_ids_ents[start_entity_id]['types']
                            start_entity_types = ann_ids_ents[end_entity_id]['types']

                            # switch ids last
                            start_entity_id = relation['end_entity']
                            end_entity_id = relation['start_entity']

                        for ent1type, ent2type in enumerate(relation_type_filter_pairs):
                            if ent1type not in start_entity_types and ent2type not in end_entity_types:
                                continue

                        ann_ids_from_reliations.extend(
                            [start_entity_id, end_entity_id])

                        relation_label = relation['relation'].strip()

                        if start_entity_id != end_entity_id and relation.get('validated', True):
                            if abs(ann_start_start_pos - ann_end_start_pos) <= self.config.general.window_size:

                                ent1_token_start_pos = [i for i in range(0, doc_length_tokens) if ann_start_start_pos
                                                        in range(tokenizer_data["offset_mapping"][i][0], tokenizer_data["offset_mapping"][i][1] + 1)][0]

                                ent2_token_start_pos = [i for i in range(0, doc_length_tokens) if ann_end_start_pos
                                                        in range(tokenizer_data["offset_mapping"][i][0], tokenizer_data["offset_mapping"][i][1] + 1)][0]

                                ent1_left_ent_context_token_pos_end = ent1_token_start_pos - \
                                    self.config.general.cntx_left

                                left_context_start_char_pos = 0
                                right_context_start_end_pos = len(text) - 1

                                if ent1_left_ent_context_token_pos_end < 0:
                                    ent1_left_ent_context_token_pos_end = 0
                                else:
                                    left_context_start_char_pos = tokenizer_data[
                                        "offset_mapping"][ent1_left_ent_context_token_pos_end][0]

                                ent2_right_ent_context_token_pos_end = ent2_token_start_pos + \
                                    self.config.general.cntx_right
                                if ent2_right_ent_context_token_pos_end >= doc_length_tokens - 1:
                                    ent2_right_ent_context_token_pos_end = doc_length_tokens - 2
                                else:
                                    right_context_start_end_pos = tokenizer_data[
                                        "offset_mapping"][ent2_right_ent_context_token_pos_end][1]

                                tmp_text = text
                                # check if a tag is present, and if not so then insert the custom annotation tags in
                                if self.config.general.annotation_schema_tag_ids[0] not in tokenizer_data["input_ids"]:
                                    _pre_e1 = text[0: (ann_start_start_pos)]
                                    _e1_s2 = text[(ann_start_end_pos): (
                                        ann_end_start_pos)]
                                    _e2_end = text[(
                                        ann_end_end_pos): len(text)]

                                    tmp_text = _pre_e1 + " " + \
                                        annotation_token_text[0] + " " + \
                                        text[ann_start_start_pos:ann_start_end_pos] + " " + \
                                        annotation_token_text[1] + " " + \
                                        _e1_s2 + " " + \
                                        annotation_token_text[2] + " " + text[ann_end_start_pos:ann_end_end_pos] + \
                                        " " + \
                                        annotation_token_text[3] + \
                                        " " + _e2_end

                                    ann_tag_token_len = len(
                                        annotation_token_text[0])

                                    _left_context_start_char_pos = left_context_start_char_pos - ann_tag_token_len - 2
                                    left_context_start_char_pos = 0 if _left_context_start_char_pos <= 0 \
                                        else _left_context_start_char_pos

                                    _right_context_start_end_pos = right_context_start_end_pos + \
                                        (ann_tag_token_len * 4) + \
                                        8  # 8 for spces
                                    right_context_start_end_pos = len(tmp_text) if right_context_start_end_pos >= len(tmp_text) or _right_context_start_end_pos >= len(tmp_text) \
                                        else _right_context_start_end_pos

                                window_tokenizer_data = self.tokenizer(
                                    tmp_text[left_context_start_char_pos:right_context_start_end_pos])

                                if self.config.general.annotation_schema_tag_ids:
                                    ent1_token_start_pos = \
                                        window_tokenizer_data["input_ids"].index(
                                            self.config.general.annotation_schema_tag_ids[0])
                                    ent2_token_start_pos = \
                                        window_tokenizer_data["input_ids"].index(
                                            self.config.general.annotation_schema_tag_ids[2])
                                else:
                                    # update token loc to match new selection
                                    ent2_token_start_pos = ent2_token_start_pos - ent1_token_start_pos
                                    ent1_token_start_pos = self.config.general.cntx_left if ent1_token_start_pos - \
                                        self.config.general.cntx_left > 0 else ent1_token_start_pos
                                    ent2_token_start_pos += ent1_token_start_pos

                                ent1_ent2_new_start = (
                                    ent1_token_start_pos, ent2_token_start_pos)
                                en1_start, en1_end = window_tokenizer_data[
                                    "offset_mapping"][ent1_token_start_pos]
                                en2_start, en2_end = window_tokenizer_data[
                                    "offset_mapping"][ent2_token_start_pos]

                                relation_instances.append([window_tokenizer_data["input_ids"], ent1_ent2_new_start, start_entity_value, end_entity_value, relation_label, self.config.model.padding_idx,
                                                           start_entity_types, end_entity_types, start_entity_id, end_entity_id, start_entity_cui, end_entity_cui, doc_id, "",
                                                           en1_start, en1_end, en2_start, en2_end])

                    output_relations.extend(relation_instances)

        all_relation_labels = [relation[4] for relation in output_relations]

        nclasses, labels2idx, idx2label = self.get_labels(
            all_relation_labels, self.config)

        # replace label_id with actual detected label number
        for idx in range(len(output_relations)):
            output_relations[idx][5] = labels2idx[output_relations[idx][4]]

        self.log.info("MCT export dataset | nclasses: " +
                      str(nclasses) + " | idx2label: " + str(idx2label))
        self.log.info("Samples per class: ")
        for label_num in list(idx2label.keys()):
            sample_count = 0
            for output_relation in output_relations:
                if int(label_num) == int(output_relation[5]):
                    sample_count += 1
            self.log.info(
                " label: " + idx2label[label_num] + " | samples: " + str(sample_count))

        return {"output_relations": output_relations, "nclasses": nclasses, "labels2idx": labels2idx, "idx2label": idx2label}

    @classmethod
    def get_labels(cls, relation_labels: List[str], config: ConfigRelCAT) -> Tuple[int, Dict[str, Any], Dict[int, Any]]:
        """ This is used to update labels in config with unencountered classes/labels ( if any are encountered during training).

        Args:
            relation_labels (List[str]): new labels to add
            config (ConfigRelCAT): config

        Returns:
            Any: _description_
        """
        curr_class_id = 0

        config_labels2idx: Dict = config.general.labels2idx
        config_idx2labels: Dict = config.general.idx2labels

        relation_labels = [relation_label.strip()
                           for relation_label in relation_labels]

        for relation_label in set(relation_labels):
            if relation_label not in config_labels2idx.keys():
                while curr_class_id in [int(label_idx) for label_idx in config_idx2labels.keys()]:
                    curr_class_id += 1
                config_labels2idx[relation_label] = curr_class_id
                config_idx2labels[curr_class_id] = relation_label

        return len(config_labels2idx.keys()), config_labels2idx, config_idx2labels,

    def __len__(self) -> int:
        """
        Returns:
            int: num of rels records
        """
        return len(self.dataset['output_relations'])

    def __getitem__(self, idx: int) -> Tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor]:
        """

        Args:
            idx (int): index of item in the dataset dict

        Returns:
            Tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor]: long tensors of the following the columns : input_ids, ent1&ent2 token start pos idx, label_ids
        """

        return torch.LongTensor(self.dataset['output_relations'][idx][0]),\
            torch.LongTensor(self.dataset['output_relations'][idx][1]),\
            torch.LongTensor([self.dataset['output_relations'][idx][5]])
