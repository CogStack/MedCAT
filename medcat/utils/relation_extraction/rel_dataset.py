from ast import literal_eval
from typing import Any, Iterable, List, Dict, Tuple, Union
from torch.utils.data import Dataset
from spacy.tokens import Doc, Span
import logging
import pandas
import random
import torch
from medcat.cdb import CDB
from medcat.config_rel_cat import ConfigRelCAT
from medcat.utils.relation_extraction.tokenizer import BaseTokenizerWrapper_RelationExtraction


class RelData(Dataset):

    name = "rel_dataset"

    log = logging.getLogger(__name__)

    def __init__(self, tokenizer: BaseTokenizerWrapper_RelationExtraction, config: ConfigRelCAT, cdb: CDB = CDB()):
        """ Use this class to create a dataset for relation annotations from CSV exports,
            MedCAT exports or Spacy Documents (assuming the documents got generated by MedCAT,
            if they did not then please set the required parameters manually to match MedCAT output,
            see /medcat/cat.py#_add_nested_ent)

            If you are using this to create relations from CSV it is assumed that your entities/concepts of
            interest are surrounded by the special tokens, see create_base_relations_from_csv doc.

        Args:
            tokenizer (BaseTokenizerWrapper_RelationExtraction): 
                tokenizer used to generate token ids from input text
            config (ConfigRelCAT): same config used in RelCAT
            cdb (CDB): Optional, used to add concept ids and types to detected ents, 
                useful when creating datasets from MedCAT output. Defaults to CDB().
        """

        self.cdb: CDB = cdb
        self.config: ConfigRelCAT = config
        self.tokenizer: BaseTokenizerWrapper_RelationExtraction = tokenizer
        self.dataset: Dict[Any, Any] = {}

        self.log.setLevel(self.config.general.log_level)

    def generate_base_relations(self, docs: Iterable[Doc]) -> List[Dict]:
        """ Util function, should be used if you want to train from spacy docs

        Args:
            docs (Iterable[Doc]): Generate relations from Spacy CAT docs.

        Returns:
            output_relations: List[Dict] : []      
                "output_relations": relation_instances, <-- see create_base_relations_from_doc/csv
                                                            for data columns
                "nclasses": self.config.model.padding_idx, <-- dummy class
                "labels2idx": {}, 
                "idx2label": {}}
                ]
        """

        output_relations = []
        for doc_id, doc in enumerate(docs):
            output_relations.append(
                self.create_base_relations_from_doc(doc, doc_id=str(doc_id),))

        return output_relations

    def create_base_relations_from_csv(self, csv_path: str, keep_source_text: bool = False):
        """
            Assumes the columns are as follows ["relation_token_span_ids", "ent1_ent2_start", "ent1", "ent2", "label",
            "label_id", "ent1_type", "ent2_type", "ent1_id", "ent2_id", "ent1_cui", "ent2_cui", "doc_id", "sents"],
            last column is the actual source text.

            The entities inside the text MUST be annotated with special tokens i.e:
                ...some text..[s1] first entity [e1].....[s2] second entity [e2]........ 
            You have to store the start position, aka index position of token [e1] and also of token [e2] in
            the (ent1_ent2_start) column.

        Args:
            csv_path (str): path to csv file, must have specific columns, tab separated,
            keep_source_text (bool): if the text clumn should be retained in the 'sents' df column,
                                    used for debugging or creating custom datasets.

        Returns:
            Dict : {  
                "output_relations": relation_instances, <-- see create_base_relations_from_doc/csv
                                                            for data columns
                "nclasses": self.config.model.padding_idx, <-- dummy class
                "labels2idx": {}, 
                "idx2label": {}}
            }
        """

        df = pandas.read_csv(csv_path, index_col=False,
                             encoding='utf-8', sep="\t")

        tmp_col_rel_token_col = df.pop("relation_token_span_ids")

        df.insert(0, "relation_token_span_ids", tmp_col_rel_token_col)

        text_cols = ["sents", "text"]

        df["ent1_ent2_start"] = df["ent1_ent2_start"].apply(
            lambda x: literal_eval(str(x)))

        for col in text_cols:
            if col in df.columns:
                out_rels = []
                for row_idx in range(len(df[col])):
                    _text = df.iloc[row_idx][col]
                    _ent1_ent2_start = df.iloc[row_idx]["ent1_ent2_start"]
                    _rels = self.create_base_relations_from_doc(
                        _text, doc_id=str(row_idx), ent1_ent2_tokens_start_pos=_ent1_ent2_start,)
                    out_rels.append(_rels)

                rows_to_remove = []
                for row_idx in range(len(out_rels)):
                    if len(out_rels[row_idx]["output_relations"]) < 1:
                        rows_to_remove.append(row_idx)

                relation_token_span_ids = []
                out_ent1_ent2_starts = []

                for rel in out_rels:
                    if len(rel["output_relations"]) > 0:
                        relation_token_span_ids.append(
                            rel["output_relations"][0][0])
                        out_ent1_ent2_starts.append(
                            rel["output_relations"][0][1])
                    else:
                        relation_token_span_ids.append([])
                        out_ent1_ent2_starts.append([])

                df["label"] = [i.strip() for i in df["label"]]

                df["relation_token_span_ids"] = relation_token_span_ids
                df["ent1_ent2_start"] = out_ent1_ent2_starts

                df = df.drop(index=rows_to_remove)
                text_col = df.pop(col)
                df = df.assign(col=text_col)
                if keep_source_text:
                    df = df.assign(col=text_col)
                break

        nclasses, labels2idx, idx2label = RelData.get_labels(
            df["label"], self.config)

        output_relations = df.values.tolist()

        self.log.info("CSV dataset | No. of relations detected:" + str(len(output_relations)) +
                      "| from : " + csv_path + " | nclasses: " + str(nclasses) + " | idx2label: " + str(idx2label))

        self.log.info("Samples per class: ")
        for label_num in list(idx2label.keys()):
            sample_count = 0
            for output_relation in output_relations:
                if idx2label[label_num] == output_relation[4]:
                    sample_count += 1
            self.log.info(
                " label: " + idx2label[label_num] + " | samples: " + str(sample_count))

        # replace/update label_id with actual detected label number
        for idx in range(len(output_relations)):
            output_relations[idx][5] = int(labels2idx[output_relations[idx][4]])

        return {"output_relations": output_relations, "nclasses": nclasses, "labels2idx": labels2idx, "idx2label": idx2label}

    def _create_relation_validation(self, 
                                    text: Union[str, Doc],
                                    doc_id: str,
                                    tokenized_text_data: Dict[str, Any],
                                    ent1_start_char_pos: int,
                                    ent2_start_char_pos: int,
                                    ent1_end_char_pos: int,
                                    ent2_end_char_pos: int,
                                    ent1_token_start_pos: int = -1,
                                    ent2_token_start_pos: int = -1,
                                    ent1_token_end_pos: int = -1,
                                    ent2_token_end_pos: int = -1,
                                    is_spacy_doc: bool = False,
                                    is_mct_export: bool = False,
                                    ) -> List:
        """
            This function checks if the relation is actually valid by distance criteria, TUIs and so on.
            Has diffierent handling cases for text, spacy docs and MCT exports.

        Args:
            text (str): doc text
            doc_id (str): doc id
            tokenized_text_data (Dict[str, Any]): tokenized text
            ent1_start_char_pos (int): ent1 start char pos
            ent2_start_char_pos (int): ent2 start char pos
            ent1_end_char_pos (int): ent1 end char pos
            ent2_end_char_pos (int): ent2 end char pos
            ent1_token_start_pos (int): ent1_token_start_pos. Defaults to -1.
            ent2_token_start_pos (int): ent2_token_start_pos. Defaults to -1.
            ent1_token_end_pos (int): ent1_token_end_pos. Defaults to -1.
            ent2_token_end_pos (int): ent2_token_end_pos. Defaults to -1.
            is_spacy_doc (bool): checks if doc is spacy docs. Defaults to False.
            is_mct_export (bool): chekcs if doc is a mct export. Defaults to False.

        Returns:
            List: row containing rel data ["relation_token_span_ids", "ent1_ent2_start", "ent1", "ent2", "label",
            "label_id", "ent1_type", "ent2_type", "ent1_id", "ent2_id", "ent1_cui", "ent2_cui", "doc_id", "sents"]
        """

        text_length:int = len(text)

        doc_token_length: int = len(tokenized_text_data["tokens"])

        tmp_doc_text = text

        ent1_token: Union[str, Span] = tmp_doc_text[ent1_start_char_pos: ent1_end_char_pos]
        ent2_token: Union[str, Span] = tmp_doc_text[ent2_start_char_pos: ent2_end_char_pos]

        if abs(ent2_start_char_pos - ent1_start_char_pos) <= self.config.general.window_size and \
             ent1_token != ent2_token:

            ent1_left_ent_context_token_pos_end = ent1_token_start_pos - self.config.general.cntx_left
            left_context_start_char_pos = 0

            if ent1_left_ent_context_token_pos_end < 0:
                ent1_left_ent_context_token_pos_end = 0
            else:
                left_context_start_char_pos = tokenized_text_data["offset_mapping"][ent1_left_ent_context_token_pos_end][0]

            ent2_right_ent_context_token_pos_end = ent2_token_end_pos + self.config.general.cntx_right
            right_context_end_char_pos = text_length

            # get correct position, don't get last token as it can be the [SEP] or [EOS] token.
            if ent2_right_ent_context_token_pos_end >= (doc_token_length - 1):
                ent2_right_ent_context_token_pos_end = doc_token_length - 2
            else:
                right_context_end_char_pos = tokenized_text_data["offset_mapping"][ent2_right_ent_context_token_pos_end][1]

            if left_context_start_char_pos > right_context_end_char_pos:
                tmp = right_context_end_char_pos
                right_context_end_char_pos = left_context_start_char_pos
                left_context_start_char_pos = tmp

            if is_spacy_doc or is_mct_export:
                tmp_doc_text = text
                _pre_e1 = tmp_doc_text[0: (ent1_start_char_pos)]
                _e1_s2 = tmp_doc_text[ent1_end_char_pos: ent2_start_char_pos - 1]
                _e2_end = tmp_doc_text[ent2_end_char_pos + 1: text_length]
                ent2_token_end_pos = (ent2_token_end_pos + 2)

                annotation_token_text = self.tokenizer.hf_tokenizers.convert_ids_to_tokens(
                                        self.config.general.annotation_schema_tag_ids)

                tmp_doc_text = str(_pre_e1) + " " + \
                                annotation_token_text[0] + " " + \
                                str(ent1_token) + " " + \
                                annotation_token_text[1] + " " + str(_e1_s2) + " " + \
                                annotation_token_text[2] + " " + str(ent2_token) + " " + \
                                annotation_token_text[3] + " " + str(_e2_end)

                ann_tag_token_len = len(annotation_token_text[0])

                _left_context_start_char_pos = left_context_start_char_pos - ann_tag_token_len - 2 # - 2 spaces
                left_context_start_char_pos = 0 if _left_context_start_char_pos <= 0 \
                    else _left_context_start_char_pos

                _right_context_start_end_pos = right_context_end_char_pos + (ann_tag_token_len * 4) + 8  # 8 for spces
                right_context_end_char_pos = len(tmp_doc_text) + 1 if right_context_end_char_pos >= len(tmp_doc_text) or \
                    _right_context_start_end_pos >= len(tmp_doc_text) else _right_context_start_end_pos

                # reassign the new text with added tags
                text_length = len(tmp_doc_text)

            # may lead to problems down the line if truncation=False, if it is True we enforce the max 512 token length sentence
            # take care when using window_size > 300, we want to make sure both entities are included at least... otherwise the relation
            # is considered invalid
            window_tokenizer_data = self.tokenizer(tmp_doc_text[left_context_start_char_pos:right_context_end_char_pos], truncation=True)

            if self.config.general.annotation_schema_tag_ids:
                try:
                    ent1_token_start_pos = \
                        window_tokenizer_data["input_ids"].index(
                            self.config.general.annotation_schema_tag_ids[0])
                    ent2_token_start_pos = \
                        window_tokenizer_data["input_ids"].index(
                            self.config.general.annotation_schema_tag_ids[2])
                    _ent1_token_end_pos = \
                        window_tokenizer_data["input_ids"].index(
                            self.config.general.annotation_schema_tag_ids[1])
                    _ent2_token_end_pos = \
                        window_tokenizer_data["input_ids"].index(
                            self.config.general.annotation_schema_tag_ids[3])
                    assert ent1_token_start_pos
                    assert ent2_token_start_pos
                    assert _ent1_token_end_pos
                    assert _ent2_token_end_pos
                except Exception as exception:
                    self.log.error("document id : " + str(doc_id) + " failed to process relation")
                    self.log.info(exception)
                    return []

            if not self.config.general.annotation_schema_tag_ids:
                # update token loc to match new selection
                ent2_token_start_pos = ent2_token_start_pos - ent1_token_start_pos
                ent1_token_start_pos = self.config.general.cntx_left if ent1_token_start_pos - self.config.general.cntx_left > 0 else ent1_token_start_pos
                ent2_token_start_pos += ent1_token_start_pos

            ent1_ent2_new_start = (ent1_token_start_pos, ent2_token_start_pos)
            en1_start, en1_end = window_tokenizer_data["offset_mapping"][ent1_token_start_pos]
            en2_start, en2_end = window_tokenizer_data["offset_mapping"][ent2_token_start_pos]

            return [window_tokenizer_data["input_ids"], ent1_ent2_new_start, ent1_token, ent2_token, "UNK", self.config.model.padding_idx,
                                            None, None, None, None, None, None, doc_id, "",
                                            en1_start, en1_end, en2_start, en2_end]
        return []

    def create_base_relations_from_doc(self, doc: Union[Doc, str], doc_id: str, ent1_ent2_tokens_start_pos: Union[List, Tuple] = (-1, -1)) -> Dict:
        """  Creates a list of tuples based on pairs of entities detected (relation, ent1, ent2) for one spacy document or text string.

        Args:
            doc (Union[Doc, str]): SpacyDoc or string of text, each will get handled slightly differently
            doc_id (str): document id
            ent1_ent2_tokens_start_pos (Union[List, Tuple], optional): start of [s1][s2] tokens, if left default
                    we assume we are dealing with a SpacyDoc. Defaults to (-1, -1).

        Returns:
                Dict : {  
                    "output_relations": relation_instances, <-- see create_base_relations_from_doc/csv
                                                                for data columns
                    "nclasses": self.config.model.padding_idx, <-- dummy class
                    "labels2idx": {}, 
                    "idx2label": {}}
                }
        """

        _ent1_start_tkn_id, _ent1_end_tkn_id, _ent2_start_tkn_id, _ent2_end_tkn_id = 0, 0, 0, 0

        chars_to_exclude = ":!@#$%^&*()-+?_=.,;<>/[]{}"

        if self.config.general.annotation_schema_tag_ids:
            # we assume that ent1 start token is pos 0 and ent2 start token is pos 2
            # e.g: [s1], [e1], [s2], [e2]
            _ent1_start_tkn_id = self.config.general.annotation_schema_tag_ids[0]
            _ent1_end_tkn_id = self.config.general.annotation_schema_tag_ids[1]
            _ent2_start_tkn_id = self.config.general.annotation_schema_tag_ids[2]
            _ent2_end_tkn_id = self.config.general.annotation_schema_tag_ids[3]

        relation_instances = []

        tokenized_text_data = None

        if isinstance(doc, str):
            doc_text = doc
        elif isinstance(doc, Doc):
            doc_text = doc.text

        tokenized_text_data = self.tokenizer(doc_text, truncation=False)

        doc_length_tokens = len(tokenized_text_data["tokens"])

        if ent1_ent2_tokens_start_pos != (-1, -1) and isinstance(doc, str):
            ent1_token_start_pos = tokenized_text_data["input_ids"].index(_ent1_start_tkn_id)
            ent2_token_start_pos = tokenized_text_data["input_ids"].index(_ent2_start_tkn_id)
            ent1_token_end_pos = tokenized_text_data["input_ids"].index(_ent1_end_tkn_id)
            ent2_token_end_pos = tokenized_text_data["input_ids"].index(_ent2_end_tkn_id)

            ent1_start_char_pos, ent1_end_char_pos = tokenized_text_data["offset_mapping"][ent1_token_start_pos]
            ent2_start_char_pos, ent2_end_char_pos = tokenized_text_data["offset_mapping"][ent2_token_start_pos]

            relation_instances.append(self._create_relation_validation(text=doc_text,
                                doc_id=doc_id,
                                tokenized_text_data=tokenized_text_data,
                                ent1_start_char_pos=ent1_start_char_pos,
                                ent2_start_char_pos=ent2_start_char_pos,
                                ent1_end_char_pos=ent1_end_char_pos,
                                ent2_end_char_pos=ent2_end_char_pos,
                                ent1_token_start_pos=ent1_token_start_pos,
                                ent2_token_start_pos=ent2_token_start_pos,
                                ent1_token_end_pos=ent1_token_end_pos,
                                ent2_token_end_pos=ent2_token_end_pos
                                ))

        elif isinstance(doc, Doc):
            _ents = doc.ents if len(doc.ents) > 0 else doc._.ents

            # last two can be a pair
            for ent1_idx in range(0, len(_ents) - 2):
                ent1_token: Span = _ents[ent1_idx]   # type: ignore

                if str(ent1_token) not in chars_to_exclude and str(ent1_token) not in self.tokenizer.hf_tokenizers.all_special_tokens:
                    ent1_type_id = list(self.cdb.cui2type_ids.get(ent1_token._.cui, ''))
                    ent1_types = [self. cdb.addl_info["type_id2name"].get(tui, '') for tui in ent1_type_id]

                    ent1_start_char_pos = ent1_token.start_char
                    ent1_end_char_pos = ent1_token.end_char

                    ent1_token_start_pos = [i for i in range(0, doc_length_tokens) if ent1_start_char_pos
                                                in range(tokenized_text_data["offset_mapping"][i][0], tokenized_text_data["offset_mapping"][i][1] + 1)][0]
                    ent1_token_end_pos = [i for i in range(0, doc_length_tokens) if ent1_end_char_pos
                                                in range(tokenized_text_data["offset_mapping"][i][0], tokenized_text_data["offset_mapping"][i][1] + 1)][0]

                    for ent2_idx in range((ent1_idx + 1), len(_ents) - 1):
                        ent2_token: Span = _ents[ent2_idx]

                        tmp_ent1 = ent1_token

                        if ent1_token.start_char > ent2_token.start_char:
                            tmp_ent1 = ent1_token
                            ent1_token = ent2_token
                            ent2_token = tmp_ent1

                        if str(ent2_token) not in chars_to_exclude and \
                                str(ent2_token) not in self.tokenizer.hf_tokenizers.all_special_tokens and \
                                        str(ent1_token).strip() != str(ent2_token).strip():

                            ent2_type_id = list(self.cdb.cui2type_ids.get(ent2_token._.cui, ''))
                            ent2_types = [self.cdb.addl_info['type_id2name'].get(tui, '') for tui in ent2_type_id]

                            ent2_start_char_pos = ent2_token.start_char
                            ent2_end_char_pos = ent2_token.end_char

                            ent2_token_start_pos = [i for i in range(0, doc_length_tokens) if ent2_start_char_pos
                                                        in range(tokenized_text_data["offset_mapping"][i][0], tokenized_text_data["offset_mapping"][i][1] + 1)][0]

                            ent2_token_end_pos = [i for i in range(0, doc_length_tokens) if ent2_end_char_pos
                                                in range(tokenized_text_data["offset_mapping"][i][0], tokenized_text_data["offset_mapping"][i][1] + 1)][0]

                            if self.config.general.relation_type_filter_pairs:
                                for rel_pair in self.config.general.relation_type_filter_pairs:
                                    if rel_pair[0] in ent1_types and rel_pair[1] in ent2_types:
                                        relation_instances.append(self._create_relation_validation(text=doc_text,
                                                doc_id=doc_id,
                                                tokenized_text_data=tokenized_text_data,
                                                ent1_start_char_pos=ent1_start_char_pos,
                                                ent2_start_char_pos=ent2_start_char_pos,
                                                ent1_end_char_pos=ent1_end_char_pos,
                                                ent2_end_char_pos=ent2_end_char_pos,
                                                ent1_token_start_pos=ent1_token_start_pos,
                                                ent2_token_start_pos=ent2_token_start_pos,
                                                ent1_token_end_pos=ent1_token_end_pos,
                                                ent2_token_end_pos=ent2_token_end_pos,
                                                is_spacy_doc=True
                                        ))
                            else:
                                relation_instances.append(self._create_relation_validation(text=doc_text,
                                                    doc_id=doc_id,
                                                    tokenized_text_data=tokenized_text_data,
                                                    ent1_start_char_pos=ent1_start_char_pos,
                                                    ent2_start_char_pos=ent2_start_char_pos,
                                                    ent1_end_char_pos=ent1_end_char_pos,
                                                    ent2_end_char_pos=ent2_end_char_pos,
                                                    ent1_token_start_pos=ent1_token_start_pos,
                                                    ent2_token_start_pos=ent2_token_start_pos,
                                                    ent1_token_end_pos=ent1_token_end_pos,
                                                    ent2_token_end_pos=ent2_token_end_pos,
                                                    is_spacy_doc=True
                                    ))

                        # restore ent1
                        ent1_token = tmp_ent1

        # remove duplicates by using ent1_ent2_start_pos
        dupe_ent1_ent2_start = []

        _new_rel_instances = []
        for rel in relation_instances:
            if rel != []:
                if rel[1] not in dupe_ent1_ent2_start:
                    dupe_ent1_ent2_start.append(rel[1])
                    _new_rel_instances.append(rel)
                else:
                    self.log.debug("removing duplicate relation" + str(rel[1]))

        # cleanup
        relation_instances = _new_rel_instances

        return {"output_relations": relation_instances, "nclasses": self.config.model.padding_idx, "labels2idx": {}, "idx2label": {}}

    def create_relations_from_export(self, data: Dict):
        """  
            Args:
                data (Dict):
                    MedCAT Export data.

            Returns:
                Dict : {  
                    "output_relations": relation_instances, <-- see create_base_relations_from_doc/csv
                                                                for data columns
                    "nclasses": self.config.model.padding_idx, <-- dummy class
                    "labels2idx": {}, 
                    "idx2label": {}}
                }
        """

        output_relations = []

        for project in data["projects"]:
            for _doc_id, document in enumerate(project["documents"]):
                doc_text: str = str(document["text"])
                doc_id: str = str(document["id"])

                if len(doc_text) > 0:
                    annotations = document["annotations"]
                    relations = document["relations"]

                    if self.config.general.lowercase:
                        doc_text = doc_text.lower()

                    tokenizer_text_data = self.tokenizer(doc_text, truncation=False)

                    doc_token_length = len(tokenizer_text_data["tokens"])

                    relation_instances = []
                    ann_ids_from_relations = []

                    ann_ids_ents: Dict[Any, Any] = {}

                    _other_relations_subset = []

                    # this section creates 'Other' class relations based on validated annotations
                    for ent1_idx, ent1_ann in enumerate(annotations):
                        ann_id = ent1_ann["id"]
                        ann_ids_ents[ann_id] = {}
                        ann_ids_ents[ann_id]["cui"] = ent1_ann["cui"]
                        ann_ids_ents[ann_id]["type_ids"] = list(self.cdb.cui2type_ids.get(ent1_ann["cui"], ""))
                        ann_ids_ents[ann_id]["types"] = [self.cdb.addl_info['type_id2name'].get(tui, '') for tui in ann_ids_ents[ann_id]['type_ids']]

                        ent1_types = ann_ids_ents[ann_id]["types"]

                        if self.config.general.create_addl_rels:
                            for _, ent2_ann in enumerate(annotations[ent1_idx + 1:]):
                                ent2_types = list(self.cdb.cui2type_ids.get(ent2_ann["cui"], ""))

                                if ent1_ann["validated"] and ent2_ann["validated"]:
                                    _relation_type = "Other"

                                    # create new Other subclass class if enabled
                                    if self.config.general.create_addl_rels_by_type:
                                        _relation_type = "Other" + ent1_types[0] + "-" + ent2_types[0]

                                    _other_relations_subset.append({
                                        "start_entity": ent1_ann["id"],
                                        "start_entity_cui": ent1_ann["cui"],
                                        "start_entity_value": ent1_ann["value"],
                                        "start_entity_start_idx": ent1_ann["start"],
                                        "start_entity_end_idx": ent1_ann["end"],
                                        "end_entity": ent2_ann["id"],
                                        "end_entity_cui": ent2_ann["cui"],
                                        "end_entity_value": ent2_ann["value"],
                                        "end_entity_start_idx": ent2_ann["start"],
                                        "end_entity_end_idx": ent2_ann["end"],
                                        "relation": _relation_type,
                                        "validated": True
                                    })

                    non_rel_sample_size_limit = int(int(self.config.general.addl_rels_max_sample_size) / len(data['projects']))

                    if non_rel_sample_size_limit > 0 and len(_other_relations_subset) > 0:
                        random.shuffle(_other_relations_subset)
                        _other_relations_subset = _other_relations_subset[0:non_rel_sample_size_limit]

                    relations.extend(_other_relations_subset)

                    for relation in relations:
                        ann_start_start_pos = relation['start_entity_start_idx']
                        ann_start_end_pos = relation["start_entity_end_idx"]

                        ann_end_start_pos = relation['end_entity_start_idx']
                        ann_end_end_pos = relation["end_entity_end_idx"]

                        start_entity_value = relation['start_entity_value']
                        end_entity_value = relation['end_entity_value']

                        start_entity_id = relation['start_entity']
                        end_entity_id = relation['end_entity']

                        start_entity_types = ann_ids_ents[start_entity_id]['types']
                        end_entity_types = ann_ids_ents[end_entity_id]['types']
                        start_entity_cui = ann_ids_ents[start_entity_id]['cui']
                        end_entity_cui = ann_ids_ents[end_entity_id]['cui']

                        # if somehow the annotations belong to the same relation but make sense in reverse
                        if ann_start_start_pos > ann_end_start_pos:
                            ann_end_start_pos = relation['start_entity_start_idx']
                            ann_end_end_pos = relation['start_entity_end_idx']

                            ann_start_start_pos = relation['end_entity_start_idx']
                            ann_start_end_pos = relation['end_entity_end_idx']

                            end_entity_value = relation['start_entity_value']
                            start_entity_value = relation['end_entity_value']

                            end_entity_cui = ann_ids_ents[start_entity_id]['cui']
                            start_entity_cui = ann_ids_ents[end_entity_id]['cui']

                            end_entity_types = ann_ids_ents[start_entity_id]['types']
                            start_entity_types = ann_ids_ents[end_entity_id]['types']

                            # switch ids last
                            start_entity_id = relation['end_entity']
                            end_entity_id = relation['start_entity']

                        for ent1type, ent2type in enumerate(self.config.general.relation_type_filter_pairs):
                            if ent1type not in start_entity_types and ent2type not in end_entity_types:
                                continue

                        ann_ids_from_relations.extend([start_entity_id, end_entity_id])
                        relation_label = relation['relation'].strip()

                        try:

                            ent1_token_start_pos = [i for i in range(0, doc_token_length) if ann_start_start_pos
                                                            in range(tokenizer_text_data["offset_mapping"][i][0], tokenizer_text_data["offset_mapping"][i][1] + 1)][0]

                            ent2_token_start_pos = [i for i in range(0, doc_token_length) if ann_end_start_pos
                                                            in range(tokenizer_text_data["offset_mapping"][i][0], tokenizer_text_data["offset_mapping"][i][1] + 1)][0]

                            ent1_token_end_pos = [i for i in range(0, doc_token_length) if ann_start_end_pos
                                                            in range(tokenizer_text_data["offset_mapping"][i][0], tokenizer_text_data["offset_mapping"][i][1] + 1)][0]

                            ent2_token_end_pos = [i for i in range(0, doc_token_length) if ann_end_end_pos
                                                            in range(tokenizer_text_data["offset_mapping"][i][0], tokenizer_text_data["offset_mapping"][i][1] + 1)][0]
                            assert ent1_token_start_pos
                            assert ent2_token_start_pos
                            assert ent1_token_end_pos
                            assert ent2_token_end_pos 
                        except Exception:
                            self.log.info("document id: " + str(doc_id) + " failed to process relation")
                            continue

                        if start_entity_id != end_entity_id and relation.get("validated", True) and start_entity_value not in self.tokenizer.hf_tokenizers.all_special_tokens \
                                and end_entity_value not in self.tokenizer.hf_tokenizers.all_special_tokens:
                            final_relation = self._create_relation_validation(text=doc_text,
                                    doc_id=doc_id,
                                    tokenized_text_data=tokenizer_text_data,
                                    ent1_start_char_pos=ann_start_start_pos,
                                    ent2_start_char_pos=ann_end_start_pos,
                                    ent1_end_char_pos=ann_start_end_pos,
                                    ent2_end_char_pos=ann_end_end_pos,
                                    ent1_token_start_pos=ent1_token_start_pos,
                                    ent2_token_start_pos=ent2_token_start_pos,
                                    ent1_token_end_pos=ent1_token_end_pos,
                                    ent2_token_end_pos=ent2_token_end_pos,
                                    is_mct_export=True
                            )

                            if len(final_relation) > 0:
                                final_relation[4] = relation_label
                                final_relation[6] = start_entity_types
                                final_relation[7] = end_entity_types
                                final_relation[8] = start_entity_id
                                final_relation[9] = end_entity_id
                                final_relation[10] = start_entity_cui
                                final_relation[11] = end_entity_cui

                                relation_instances.append(final_relation)

                    output_relations.extend(relation_instances)

        all_relation_labels = [relation[4] for relation in output_relations]

        nclasses, labels2idx, idx2label = self.get_labels(
            all_relation_labels, self.config)

        # replace label_id with actual detected label number
        for idx in range(len(output_relations)):
            output_relations[idx][5] = int(labels2idx[output_relations[idx][4]])

        self.log.info("MCT export dataset | nclasses: " +
                      str(nclasses) + " | idx2label: " + str(idx2label))
        self.log.info("Samples per class: ")

        self.log.error(str(idx2label))

        for label_num in list(idx2label.keys()):
            sample_count = 0
            for output_relation in output_relations:
                if idx2label[label_num] == output_relation[4]:
                    sample_count += 1
            self.log.info(
                " label: " + idx2label[label_num] + " | samples: " + str(sample_count))

        return {"output_relations": output_relations, "nclasses": nclasses, "labels2idx": labels2idx, "idx2label": idx2label}

    @classmethod
    def get_labels(cls, relation_labels: List[str], config: ConfigRelCAT) -> Tuple[int, Dict[str, int], Dict[int, str]]:
        """ This is used to update labels in config with unencountered classes/labels ( if any are encountered during training).

        Args:
            relation_labels (List[str]): new labels to add
            config (ConfigRelCAT): config

        Returns:
            Tuple[int, Dict[str, int], Dict[int, str]]: label count, labesl2idx mapping, idx2labels mapping
        """
        curr_class_id = 0

        config_labels2idx: Dict[str, int] = config.general.labels2idx
        config_idx2labels: Dict[int, str] = config.general.idx2labels

        relation_labels = [relation_label.strip()
                           for relation_label in relation_labels]

        for relation_label in set(relation_labels):
            if relation_label not in config_labels2idx.keys():
                while curr_class_id in [int(label_idx) for label_idx in config_idx2labels.keys()]:
                    curr_class_id += 1
                config_labels2idx[relation_label] = int(curr_class_id)
                config_idx2labels[int(curr_class_id)] = relation_label

        return len(config_labels2idx.keys()), config_labels2idx, config_idx2labels,

    def __len__(self) -> int:
        """
        Returns:
            int: num of rels records
        """
        return len(self.dataset['output_relations'])

    def __getitem__(self, idx: int) -> Tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor]:
        """

        Args:
            idx (int): index of item in the dataset dict

        Returns:
            Tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor]: long tensors of the following the columns : input_ids, ent1&ent2 token start pos idx, label_ids
        """

        return torch.LongTensor(self.dataset['output_relations'][idx][0]),\
            torch.LongTensor(self.dataset['output_relations'][idx][1]),\
            torch.LongTensor([self.dataset['output_relations'][idx][5]])
